{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8a84bce5-4d14-467c-a553-1297c3505ce8"
    }
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7f080332-843a-4a2d-9cbd-d092c5e48670"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from difflib import context_diff\n",
    "from pprint import pprint\n",
    "import sqlalchemy\n",
    "from datetime import datetime as dt\n",
    "from os import getcwd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8d4c7aed-e1c6-403f-8e08-a4e99c0fab7e"
    }
   },
   "source": [
    "# Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "dab62920-d3dc-425d-b991-73b769cf6c6b"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Helper functions #\n",
    "####################\n",
    "\n",
    "def splitbynewlines(tosplit):\n",
    "    remr = []\n",
    "    toret = []\n",
    "    if isinstance(tosplit, basestring):\n",
    "        remn = [tosplit]\n",
    "    else:\n",
    "        remn = tosplit\n",
    "    for elem in remn:\n",
    "        remr = remr + elem.split('\\n')\n",
    "    for elem in remr:\n",
    "        toret = toret + elem.split('\\r')\n",
    "    return toret\n",
    "\n",
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "def catcols(df):\n",
    "    catcols = [c for c in df.select_dtypes(include=['object']).columns] + \\\n",
    "            [c for c in df.columns if c[-2:].lower() == 'yn'] + \\\n",
    "            [c for c in df.columns if c[-2:].lower() == 'id'] + \\\n",
    "            [c for c in df.columns[df.isin([0,1]).all()]]\n",
    "    # Remove duplicates\n",
    "    return dedup(catcols)\n",
    "\n",
    "def sumsumsqdiff(x,df):\n",
    "    toret = 0\n",
    "    for i, row in df.iterrows():\n",
    "        toret = toret + ((x - row)**2).sum()\n",
    "    return toret\n",
    "\n",
    "###########\n",
    "# Classes #\n",
    "###########\n",
    "\n",
    "class FindAssumptionBreakers:\n",
    "    '''\n",
    "    This class allows you to specify certain assumptions you have about your data,\n",
    "    then surfaces rows for you to review, in order of how many of your assumptions they broke.\n",
    "    '''\n",
    "    def __init__(self,df,identifier):\n",
    "\n",
    "        if identifier not in df.columns:\n",
    "            raise KeyError(\"Specified unique identifier '{}' is not a column of the passed dataframe.\".format(identifier))\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        # Results that will be returned:\n",
    "        self.res = pd.DataFrame(self.df[identifier]).copy()\n",
    "        \n",
    "        # A place for applied functions to store errors, overriding each time, to avoid an error per row\n",
    "        self.errors = ''\n",
    "        \n",
    "        # How many example rows have been shown so far?\n",
    "        self.eg = 0\n",
    "        \n",
    "        # For the max/min range tests:\n",
    "        self.ltgt = (operator.lt,operator.gt)\n",
    "        \n",
    "        # Identify columns by dtype:\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        dates = ['<M8', 'datetime64']\n",
    "        numericcols = [c for c in self.df.select_dtypes(include=numerics).columns]\n",
    "        datecols = [c for c in self.df.select_dtypes(include=dates).columns]\n",
    "        bothcolgroups = numericcols + datecols\n",
    "        \n",
    "        # Create parameters:\n",
    "        numdict = zip(numericcols,[(0,1000)] * len(numericcols))\n",
    "        datedict = zip(datecols,[('2002-01-01',dt.now().date().isoformat())] * len(datecols))\n",
    "        self.ranges = dict(numdict + datedict)\n",
    "        \n",
    "        # Need these for later\n",
    "        self.pattern = re.compile('|'.join(bothcolgroups))\n",
    "        self.cannull = []\n",
    "        self.tests = []\n",
    "        \n",
    "        # Are these new parameterS?\n",
    "        self.newparams = False\n",
    "    \n",
    "    def parameterTemplate(self):\n",
    "        m = []\n",
    "        m.append('cannull = [\\n')\n",
    "        # Print columns commented out, to allow easy additition to cannull list:\n",
    "        cols = []\n",
    "        for col in self.df.columns:\n",
    "            comment = (\"#\" if not self.df[col].isnull().all() else \"\")\n",
    "            cols += [comment + \"'\" + col + \"'\"]\n",
    "        m.append(',\\n'.join(cols))\n",
    "        m.append('\\n]\\n\\nranges = ' + str(self.ranges).replace('),','),\\n'))\n",
    "        # Example tests\n",
    "        m.append(\"\\n\\ntests = [#'A <= B',\\n#'A + B * 2 = C'\\n]\")\n",
    "        print(''.join(m))\n",
    "        \n",
    "    def TESTparameterTemplate(self):\n",
    "        m = []\n",
    "        m.append('cannull = [\\n')\n",
    "        # Print columns commented out, to allow easy additition to cannull list:\n",
    "        m.append(',\\n'.join([\"#'\" + x + \"'\" for x in self.df.columns]))\n",
    "        m.append('\\n]\\n\\nranges = ')\n",
    "        ranges = str(self.ranges)\n",
    "        ranges = ranges.replace('{','{\\n')\n",
    "        ranges = ranges.replace('}','\\n}')\n",
    "        ranges = ranges.replace('),','),\\n')\n",
    "        for line in ranges.split('\\n'):\n",
    "            if \"id':\" in line:\n",
    "                m.append('# ')\n",
    "            elif \"yn':\" in line:\n",
    "                m.append('# ')\n",
    "            m.append(line + '\\n')\n",
    "        # Example tests\n",
    "        m.append(\"\\n\\ntests = [#'A <= B',\\n#'A + B * 2 = C'\\n]\")\n",
    "        print(''.join(m))\n",
    "    \n",
    "    def updateParameters(self,cannull=None,ranges=None,tests=None):\n",
    "        if cannull:\n",
    "            self.cannull = cannull\n",
    "        if tests:\n",
    "            self.tests = tests\n",
    "        if ranges:\n",
    "            self.ranges = ranges\n",
    "        if any([cannull,tests,ranges]):\n",
    "            self.newparams = True\n",
    "        \n",
    "    def inc(self,series,msg):\n",
    "        # Increment the count of broken invariants for rows that broke this invariant\n",
    "        ones = series * 1\n",
    "        self.res['CNT'] = self.res['CNT'] + ones\n",
    "        # Add the error message to the messages field\n",
    "        msgs = series.replace(True,msg + '; ').replace(False,'')\n",
    "        self.res['MSGS'] = self.res['MSGS'].str.cat(msgs)\n",
    "        \n",
    "    def nullTest(self,col):\n",
    "        if col not in self.cannull and self.df[col].isnull().values.any(): # For columns that aren't ok to be null, but have a null,\n",
    "            self.inc(self.df[col].isnull(),'nullTest: ' + col) # Add one to the variant count for each null row\n",
    "    \n",
    "    def rangeTest(self,col):\n",
    "        if col in self.ranges.keys(): # For each column that we know the 'valid' range of\n",
    "            for comp in [0,1]: # Check if less than min or greater than max,\n",
    "                self.inc(self.ltgt[comp](self.df[col],self.ranges[col][comp]),'rangeTest: ' + col) # And add 1 to each row where true\n",
    "\n",
    "    def evalTest(self):\n",
    "        for t in self.tests:\n",
    "            self.inc(self.df.apply(self.testRow,axis=1,test=t),'evalTest: ' + t)\n",
    "            \n",
    "    def testRow(self,row,test):\n",
    "        toeval = self.pattern.sub(lambda x: \"row['\" + x.group() + \"']\", test)\n",
    "        try:\n",
    "            evalres = not eval(toeval)\n",
    "        except:\n",
    "            self.errors = 'Invalid Test: ' + test + ' --> ' + toeval + '\\n'\n",
    "            evalres = False\n",
    "        return evalres\n",
    "    \n",
    "    def testassumptions(self):\n",
    "        # Initialize:\n",
    "        self.res['CNT'] = pd.Series(0, index=self.res.index)\n",
    "        self.res['MSGS'] = pd.Series('', index=self.res.index)\n",
    "        \n",
    "        # Run tests\n",
    "        for col in self.df.columns:\n",
    "            self.nullTest(col)\n",
    "            self.rangeTest(col)\n",
    "        self.evalTest()\n",
    "        \n",
    "        # Record state\n",
    "        if self.newparams:\n",
    "            self.eg = 0\n",
    "        self.newparams = False\n",
    "            \n",
    "    def assumptionbreaker(self):\n",
    "        # Sort the data by the number of invariants broken, descending:\n",
    "        merged = pd.merge(self.res,self.df).sort_values('CNT',ascending=False)\n",
    "        # Get the first row not yet displayed\n",
    "        todisp = merged.iloc[self.eg]\n",
    "        # Update self.eg to be the index of examples not yet seen\n",
    "        self.eg = self.eg + 1\n",
    "        if todisp['CNT'] == 0:\n",
    "            print('No assumptions are broken.')\n",
    "        else:\n",
    "            # Display the relevant rows\n",
    "            print('This row broke {} assumptions:\\n{}'.format(todisp['CNT'],todisp['MSGS']))\n",
    "            display(todisp[[x for x in todisp.index if x not in ['CNT','MSGS']]].T)\n",
    "            \n",
    "            \n",
    "    def showErrors(self):\n",
    "        print(self.errors)\n",
    "            \n",
    "class FindUnusualExamples:\n",
    "    '''\n",
    "    This class surfaces rows for you to review in order of how dissimilar they are to every other row\n",
    "    in the dataframe. Dissimilarity is measured as the distance from the median for continuous variables,\n",
    "    and for categorical variables is measured as rarity compared to the most common category.\n",
    "    '''\n",
    "    def __init__(self,df,exampleid):\n",
    "        \n",
    "        if exampleid not in df.columns:\n",
    "            raise KeyError(\"Specified unique identifier '{}' is not a column of the passed dataframe.\".format(exampleid))\n",
    "        \n",
    "        self.df = df\n",
    "        \n",
    "        # Results that will be returned:\n",
    "        self.res = pd.DataFrame(self.df[exampleid]).copy()\n",
    "        \n",
    "        # Example ID\n",
    "        self.egid = exampleid\n",
    "    \n",
    "    def catDeviScore(self,col):\n",
    "        # Find the % of the data in each category\n",
    "        self.res[col] = self.df[col].values\n",
    "        dist = pd.value_counts(self.df[col].values,normalize=True)\n",
    "        if dist.empty:\n",
    "            return\n",
    "        # We will give categories with the highest frequency a score of 0, and\n",
    "        # the categories with the lowest frequency a score of 1\n",
    "        dist2 = dedup(dist.values)\n",
    "        dist3 = [dist2[0] / x if x != 0 else 0 for x in dist2]\n",
    "        dist4 = [x - 1 for x in dist3]\n",
    "        dist5 = [x / dist4[-1] if dist4[-1] != 0 else 0 for x in dist4]\n",
    "        dist6 = pd.DataFrame(dist2,dist5).reset_index()\n",
    "        # Mapping series\n",
    "        mapser = pd.merge(pd.DataFrame(dist).reset_index(),dist6,on=0)\n",
    "        mapdict = mapser[['index_x','index_y']].set_index('index_x').to_dict()['index_y']\n",
    "        self.res[col] = self.res[col].map(mapdict)\n",
    "        \n",
    "    def contDeviScore(self,theseries,col):\n",
    "        percen = theseries.map(lambda x: stats.percentileofscore(theseries,x,kind='mean'))\n",
    "        self.res[col] = percen.map(lambda x: 2 * abs(50 - x) / 100)\n",
    "    \n",
    "    def numDeviScore(self,col):\n",
    "        self.contDeviScore(self.df[col],col)\n",
    "    \n",
    "    def dateDeviScore(self,col):\n",
    "        seconds = self.df[col].map(lambda x: (x - dt.fromtimestamp(0)).total_seconds())\n",
    "        self.contDeviScore(seconds,col)\n",
    "    \n",
    "    def parameterTemplate(self):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        dates = ['<M8', 'datetime64']\n",
    "        typedict = {}\n",
    "        catcols = catcols(self.df)\n",
    "        typedict['categorical'] = catcols\n",
    "        numcols = [c for c in self.df.select_dtypes(include=numerics).columns if c not in catcols]\n",
    "        typedict['numbers'] = numcols\n",
    "        datecols = [c for c in self.df.select_dtypes(include=dates).columns if c not in catcols]\n",
    "        typedict['dates'] = datecols\n",
    "        nullcols = [x for x in self.df.columns if self.df[x].isnull().all()]\n",
    "\n",
    "        print('parameters = ' + str(typedict).replace(',',',\\n'))\n",
    "    \n",
    "    def initialize(self,typedict):\n",
    "        if 'dates' in typedict:\n",
    "            for col in typedict['dates']:\n",
    "                self.dateDeviScore(col)\n",
    "        if 'numbers' in typedict:\n",
    "            for col in typedict['numbers']:\n",
    "                self.numDeviScore(col)\n",
    "        if 'categorical' in typedict:\n",
    "            for col in typedict['categorical']:\n",
    "                self.catDeviScore(col)\n",
    "    \n",
    "    def example(self):\n",
    "        percentiles = [x for x in self.res.columns if x not in [self.egid,'Score']]\n",
    "        \n",
    "        if not hasattr(self, 'prevvec'):\n",
    "            self.prevvec = pd.DataFrame(0,index=[0],columns=percentiles)\n",
    "        \n",
    "        # Add sqrt(sum(squared(differences))) to score for that example\n",
    "        self.res['Score'] = self.res.apply(sumsumsqdiff,axis=1,args=(self.prevvec,))\n",
    "        \n",
    "        # Index of row with the highest score:\n",
    "        highid = self.res['Score'].idxmax()\n",
    "        \n",
    "        # Example ID of the first row with the highest score:\n",
    "        weirdest = self.res.loc[highid][self.egid]\n",
    "        \n",
    "        # Display row from the original dataframe identified by that exampleid\n",
    "        rowtoshow = self.df.loc[self.df[self.egid] == weirdest].T\n",
    "        rowtoshow.columns = ['Values']*len(rowtoshow.columns)\n",
    "        \n",
    "        # And show the uniqueness scores so it's clear why the example was chosen\n",
    "        rowtoshow['Uniqueness'] = self.res.loc[highid].drop(['uniqueid','Score'])\n",
    "        rowtoshow['Uniqueness'] = rowtoshow['Uniqueness'].fillna('N/A')\n",
    "        \n",
    "        print('The following row is the most unique, out of the rows you have not yet reviewed:')\n",
    "        display(rowtoshow)\n",
    "        print('The uniqueness scores are 0 for median values and the most common categories, and 1 is for mins, maxs, and the least common categories.')\n",
    "        \n",
    "        # Add that example to the previous vector array\n",
    "        self.prevvec = self.prevvec.append(self.res.loc[highid][percentiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "94f30ff0-7f6b-4607-a9b9-c6d7d841b47d"
    }
   },
   "source": [
    "# Input parameters\n",
    "\n",
    "Enter the name of your query file, a context you want to run on, the number of examples you want, and the name of the column that contains a unique identifier for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('You are currently in ' + getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "05441210-7072-451a-9958-cb9664d17b25"
    }
   },
   "outputs": [],
   "source": [
    "# What is the name of your query file?\n",
    "queryfile = 'EXAMPLE.sql'\n",
    "\n",
    "# How many examples do you need, minimum?\n",
    "numegs = 200\n",
    "\n",
    "# How many days do you want to run the query over at first?\n",
    "initialdays = 1\n",
    "\n",
    "# What is the name of the column contains a unique identifier?\n",
    "# WARNING! If you put something here that's not truly unique, it will cause subtle errors later\n",
    "uniqueid = 'id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the database\n",
    "\n",
    "Get your database handle, dbh, here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5c97bdee-f6b9-4f8a-a258-6041d0b31834"
    }
   },
   "outputs": [],
   "source": [
    "# Put the code here you need to get your database handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "58d02874-e648-48b5-924b-6fc113b015fd"
    }
   },
   "source": [
    "# Difference from the last query\n",
    "\n",
    "What were your most recent changes to your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1c615f8a-b9fb-4587-bde0-afcfb69b2abb"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'oldquery' in locals():\n",
    "    oldoldquery = oldquery # Back up the backup\n",
    "\n",
    "if 'query' in locals():\n",
    "    oldquery = query # Back up the present\n",
    "\n",
    "# Open the query\n",
    "with open(queryfile, 'r') as f:\n",
    "    query = f.read()\n",
    "\n",
    "if 'oldoldquery' in locals() and query == oldquery: # If there was no change,\n",
    "    oldquery = oldoldquery # the diff before that will be more interesting\n",
    "\n",
    "# Show what last changed about the query\n",
    "if 'oldquery' in locals() and 'query' in locals():\n",
    "    for line in context_diff(splitbynewlines(oldquery),splitbynewlines(query)):\n",
    "        pprint(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fc4031e8-6026-4746-b89a-ea5c247c2385"
    }
   },
   "source": [
    "# Run the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c6dd1c8d-561c-4756-a6ef-14be44bb7115"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "if ':days' not in friendlyq: # If there is no day span specified, just run it\n",
    "    try:\n",
    "        df = pd.read_sql(friendlyq,dbh)\n",
    "    except Exception as exception:\n",
    "        print(exception.orig)\n",
    "        print(exception.statement)\n",
    "    numrows = len(df)\n",
    "    print('Finished: Pulled {} rows, in {:.2f} seconds.'.format(numrows,time.time()-starttime))\n",
    "else: # If there is a dayspan, run it over increasingly larger dayspans until you get enough rows\n",
    "    daysspan = initialdays\n",
    "    print('Running query over {:.2f} days'.format(daysspan))\n",
    "    try:\n",
    "        df = pd.read_sql(friendlyq,dbh,params={'days':daysspan})\n",
    "    except Exception as exception:\n",
    "        print(exception.orig)\n",
    "        print(exception.statement)\n",
    "    numrows = len(df)\n",
    "\n",
    "    # Did we get enough examples?\n",
    "    while numrows < numegs:\n",
    "        try:\n",
    "            # How far from the number of rows we want are we?\n",
    "            faroff = float(numegs) / numrows\n",
    "        except ZeroDivisionError:\n",
    "            faroff = 2\n",
    "\n",
    "        # Let's increase the search, but not go up by more than two, to be careful not to overadjust\n",
    "        daysspan *= min(faroff * 1.1,2)\n",
    "\n",
    "        if daysspan > 365:\n",
    "            print(\"Stopping, on the assumption you don't want to run this over more than a year.\")\n",
    "            break\n",
    "\n",
    "        print('Only got {} rows. Now running query over {:.2f} days'.format(numrows,daysspan))\n",
    "        df = pd.read_sql(friendlyq,dbh,params={'days':daysspan})\n",
    "        numrows = len(df)\n",
    "\n",
    "    print('Finished: Pulled {} rows, from {:.2f} days, in {:.2f} seconds.'.format(numrows,daysspan,time.time()-starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e333eb5e-97bc-4c7a-ac9f-3293e281b5de"
    }
   },
   "source": [
    "# Take a high level look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "232c3695-0a70-489d-a897-bde7dbfb01f3"
    }
   },
   "source": [
    "### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2812b6ac-eb92-414c-b639-150827143e7f"
    }
   },
   "outputs": [],
   "source": [
    "# Show me the top N values in each category:\n",
    "topN = 5\n",
    "\n",
    "nullcols = []\n",
    "for x in catcols(df):\n",
    "    todisp = pd.value_counts(df[x].values,normalize=True).iloc[:topN]\n",
    "    if not todisp.empty:\n",
    "        display(pd.DataFrame(todisp.rename(str(x)).map(lambda x: \"{0:.4f} %\".format(x * 100))))\n",
    "        print('Top {} represent {:.1%} of rows.\\n\\n'.format(topN,todisp.sum()))\n",
    "    else:\n",
    "        nullcols = nullcols + [str(x)]\n",
    "if nullcols:\n",
    "    print('The following categorical columns are entirely null:\\n')\n",
    "    for x in nullcols:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e08de992-bf5d-4c18-b364-d5ec10ed09d4"
    }
   },
   "source": [
    "### Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc = pd.DataFrame()\n",
    "for col in df:\n",
    "    s = df[col]\n",
    "    if col in catcols(df):\n",
    "        continue\n",
    "    if s.dtype not in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "        continue\n",
    "    d = {}\n",
    "    d['count'] = s.count()\n",
    "    d['sum'] = s.sum()\n",
    "    d['mean'] = s.mean()\n",
    "    numnull = s.isnull().sum() \n",
    "    d['%null'] = float(numnull) / (numnull + d['count'])\n",
    "    d['min'] = s.min()\n",
    "    d['10%'] = s.dropna().quantile(0.1)\n",
    "    d['50%'] = s.dropna().quantile(0.5)\n",
    "    d['90%'] = s.dropna().quantile(0.9)\n",
    "    d['max'] = s.max()\n",
    "    desc[col] = pd.Series(d)\n",
    "    \n",
    "def readableNumbers(x):\n",
    "    toins = ''\n",
    "    if isinstance(x, float):\n",
    "        decimalplaces = 3\n",
    "        if x.is_integer():\n",
    "            decimalplaces = 0\n",
    "        toins = '.' + str(decimalplaces) + 'f'\n",
    "    return ('{:,' + toins + '}').format(x)\n",
    "\n",
    "colorder = ['count','sum','mean','%null','min','10%','50%','90%','max']\n",
    "display(desc.applymap(readableNumbers).T[colorder].style.applymap(lambda x: 'text-align:right'))\n",
    "\n",
    "sns.pairplot(df[desc.columns].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1ff83d0f-0f41-4de1-a4db-0bd1c3a99df3"
    }
   },
   "source": [
    "# Test your assumptions about the data \n",
    "Copy the output of the next cell into the \"Put your updated parameters here\" cell, and then modify it.\n",
    "\n",
    "#### For the cannull variable:\n",
    "Remove the comment '#' from any column that can be null\n",
    "\n",
    "#### For the ranges variable:\n",
    "The default is to assume every number is between 0 and 1000 inclusive, and every date is between 2002 and today. Change those to the range you would be shocked to see those columns fall outside of.\n",
    "\n",
    "#### For the tests variable:\n",
    "Write out equations--in the same format as the examples--that you would assume are always true. Use column names instead of 'A', 'B' and 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "dcf0e7bd-78c3-4ade-b8a9-51da55c56a46"
    }
   },
   "outputs": [],
   "source": [
    "instance = FindAssumptionBreakers(df,uniqueid)\n",
    "instance.parameterTemplate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e2382c75-6c1b-4bfe-8b22-7c4933b289e6"
    }
   },
   "source": [
    "## Put your updated parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "30779af7-6579-44ba-b4b4-40e7e4f171f2"
    }
   },
   "outputs": [],
   "source": [
    "cannull = [\n",
    "#'id',\n",
    "#'event',\n",
    "#'eventdate',\n",
    "#'username',\n",
    "#'uniqueid',\n",
    "'deleted',\n",
    "'deletedby',\n",
    "#'created',\n",
    "#'createdby',\n",
    "'lastmodified',\n",
    "'lastmodifiedby',\n",
    "#'claimid',\n",
    "'claimnoteid',\n",
    "'transactionid',\n",
    "'kickreasonid'\n",
    "]\n",
    "\n",
    "ranges = {\n",
    "#     u'kickreasonid': (0, 1000),\n",
    " u'created': ('2017-01-01', '2017-10-06'),\n",
    "#  u'claimid': (0, 1000),\n",
    "#  u'id': (0, 1000),\n",
    "#  u'claimnoteid': (0, 1000),\n",
    " u'eventdate': ('2017-01-01', '2017-10-06')}\n",
    "\n",
    "tests = [\n",
    "    'eventdate <= created',\n",
    "#'A + B * 2 = C'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d7fc656d-c1f4-4fe7-9b8e-97afc0276bd0"
    }
   },
   "source": [
    "## Feed the updated parameters to the instance of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b3cab2bf-0e66-4e17-95fe-1a73b101b2b9"
    }
   },
   "outputs": [],
   "source": [
    "instance.updateParameters(cannull,ranges,tests)\n",
    "# Test all of the assumptions\n",
    "instance.testassumptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e75e2b5f-c266-4b48-aab6-c111bd7d17ce"
    }
   },
   "source": [
    "## Run this to see a row that breaks your assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "30cee2cf-3989-4de5-a51e-239a00c3448c"
    }
   },
   "outputs": [],
   "source": [
    "# Display a row that broke the assumptions\n",
    "instance.assumptionbreaker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bb5b5ab3-45b1-4f13-b6b7-69ae11fcb380"
    }
   },
   "source": [
    "# Find unusual examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "37879267-006b-4f57-877c-10373eb3c05c"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "instance = FindUnusualExamples(df,uniqueid)\n",
    "instance.parameterTemplate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any list items, or even full dictionary keys, that you don't want to be part of the unusualness tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2802589a-25da-4add-8b2c-a92cd42f032c"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'dates': [u'eventdate',\n",
    " u'created'],\n",
    " 'categorical': [u'event',\n",
    " u'username',\n",
    " u'createdby']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "3ae9b3d2-03ac-464c-bfad-f32a58c34a50"
    }
   },
   "outputs": [],
   "source": [
    "# For some reason stats.percentileofscore() throws an error if it's not imported right before being called:\n",
    "from scipy import stats\n",
    "instance.initialize(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instance.example()\n",
    "print('\\nContextid: {}'.format(contextid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Drop this data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvname = queryfile[:-3] + 'csv'\n",
    "\n",
    "print('The CSV, by default will be \"{}\".\\n'.format(csvname))\n",
    "print('To change this, uncomment and change \"example path\" below')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvname = \"Examplepath.csv\"\n",
    "\n",
    "df.to_csv(csvname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "nbpresent": {
   "slides": {
    "0f0a9438-dbfe-4380-acea-7f1815abde6a": {
     "id": "0f0a9438-dbfe-4380-acea-7f1815abde6a",
     "prev": null,
     "regions": {
      "0ab90680-c705-40fc-9fe1-025477369f83": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "7f080332-843a-4a2d-9cbd-d092c5e48670",
        "part": "source"
       },
       "id": "0ab90680-c705-40fc-9fe1-025477369f83"
      }
     }
    },
    "a8418188-5616-462b-851c-db9fa29014bd": {
     "id": "a8418188-5616-462b-851c-db9fa29014bd",
     "prev": "0f0a9438-dbfe-4380-acea-7f1815abde6a",
     "regions": {
      "117bb2cc-3b90-4322-b70a-bc040109c43c": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "dab62920-d3dc-425d-b991-73b769cf6c6b",
        "part": "source"
       },
       "id": "117bb2cc-3b90-4322-b70a-bc040109c43c"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
